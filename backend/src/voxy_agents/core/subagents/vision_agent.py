"""
Vision Subagent - OpenAI Agents SDK Implementation

Subagente especializado em anÃ¡lise visual multimodal.
Usa OpenAI Agents SDK v0.2.9 + LiteLLM Multi-Provider.

Arquitetura padronizada com outros subagentes (Calculator, Weather, etc.).
Dual-path support: Direct analysis + VOXY Orchestrator integration.

MigraÃ§Ã£o Loguru - Sprint 5
"""

import asyncio
from datetime import datetime
from typing import Optional

import nest_asyncio
from agents import Agent, ModelSettings, Runner
from loguru import logger

from ...config.models_config import load_vision_config
from ...utils.llm_factory import create_litellm_model


class VisionAgent:
    """
    Subagente de anÃ¡lise visual multimodal.

    CaracterÃ­sticas:
    - LiteLLM configurÃ¡vel para suporte a mÃºltiplos providers (OpenRouter, OpenAI, Anthropic, etc.)
    - AnÃ¡lise nativa de imagens via multimodal models
    - Dual-path: Direct analysis + VOXY Orchestrator integration
    - Arquitetura padronizada com outros subagentes
    """

    def __init__(self):
        """
        Initialize the vision subagent with configurable LiteLLM model.

        Configuration is loaded from environment variables:
        - VISION_PROVIDER: Provider name (openrouter, openai, anthropic)
        - VISION_MODEL: Model name (e.g., "openai/gpt-4o")
        - VISION_MAX_TOKENS: Max tokens (default: 2000)
        - VISION_TEMPERATURE: Temperature (default: 0.1)
        - API key for selected provider (OPENROUTER_API_KEY, OPENAI_API_KEY, etc.)

        Raises:
            ValueError: If required API key is missing or config is invalid
        """
        import time

        start_time = time.perf_counter()

        # Apply nest_asyncio patch for nested event loop compatibility
        # Required for process_tool_call() sync wrapper (VOXY integration)
        nest_asyncio.apply()

        # Load configuration from environment variables
        config = load_vision_config()

        # Create LiteLLM model instance via factory
        model = create_litellm_model(config)

        # Create agent with configured model
        self.agent = Agent(
            name="Subagente Vision VOXY",
            model=model,
            instructions=self._get_instructions(),
            model_settings=ModelSettings(
                include_usage=config.include_usage,
                temperature=config.temperature,
            ),
        )

        # Store config for reference
        self.config = config
        elapsed_ms = (time.perf_counter() - start_time) * 1000

        # Log initialization with reasoning config (equal to other agents)
        reasoning_status = "enabled" if config.reasoning_enabled else "disabled"
        reasoning_info = ""
        if config.reasoning_enabled:
            if config.thinking_budget_tokens:
                reasoning_info = f", thinking={config.thinking_budget_tokens} tokens"
            elif config.reasoning_effort:
                reasoning_info = f", effort={config.reasoning_effort}"

        logger.bind(event="STARTUP|SUBAGENT_INIT").info(
            f"\n"
            f"   â””â”€ âœ“ Vision\n"
            f"      â”œâ”€ Model: {config.get_litellm_model_path()}\n"
            f"      â”œâ”€ Provider: {config.provider}\n"
            f"      â”œâ”€ Config: {config.max_tokens} tokens, temp={config.temperature}\n"
            f"      â”œâ”€ Reasoning: {reasoning_status}{reasoning_info}\n"
            f"      â””â”€ âœ“ Ready in {elapsed_ms:.1f}ms"
        )

    def _get_instructions(self) -> str:
        """Get specialized instructions for image analysis."""
        return """
        VocÃª Ã© um subagente de anÃ¡lise visual especializado do sistema VOXY.

        CAPACIDADES PRINCIPAIS:
        - AnÃ¡lise detalhada de imagens com alta precisÃ£o
        - IdentificaÃ§Ã£o de objetos, pessoas, cenas e contextos
        - OCR integrado para extraÃ§Ã£o de texto em imagens
        - AnÃ¡lise de documentos, grÃ¡ficos, diagramas e infogrÃ¡ficos
        - DetecÃ§Ã£o de problemas tÃ©cnicos e qualidade visual
        - AnÃ¡lise artÃ­stica: composiÃ§Ã£o, cores, estilo, tÃ©cnica
        - Reconhecimento de padrÃµes e elementos visuais

        DIRETRIZES:
        - Seja preciso e especÃ­fico nas descriÃ§Ãµes
        - Identifique elementos principais primeiro
        - Para anÃ¡lises simples: resposta clara e concisa
        - Para anÃ¡lises complexas: estruture com seÃ§Ãµes lÃ³gicas
        - Use formataÃ§Ã£o markdown quando apropriado
        - Seja honesto sobre limitaÃ§Ãµes quando aplicÃ¡vel
        - Adapte nÃ­vel de detalhe ao contexto da pergunta

        FORMATO DE RESPOSTA:
        - AnÃ¡lises bÃ¡sicas: resposta direta em 1-2 parÃ¡grafos
        - AnÃ¡lises detalhadas: estruturadas com tÃ³picos ou seÃ§Ãµes
        - Sempre destaque informaÃ§Ãµes importantes
        - Use linguagem natural e acessÃ­vel

        EXEMPLOS:
        Input: "Qual emoji Ã© este?"
        Output: "Este Ã© o emoji ðŸ˜Š (cara sorridente com olhos sorridentes)"

        Input: "Descreva esta imagem em detalhes"
        Output: "# AnÃ¡lise da Imagem\\n\\n## Elementos Principais\\n- ...\\n\\n## Contexto\\n- ..."

        Input: "Extraia o texto desta imagem"
        Output: "Texto extraÃ­do:\\n1. ...\\n2. ..."
        """

    async def analyze_image(
        self,
        image_url: str,
        query: str = "Analise esta imagem",
        user_id: str = "default",
    ) -> str:
        """
        Analyze image with Vision model.

        Args:
            image_url: URL or base64 data of the image
            query: User's question about the image
            user_id: User identifier (for logging)

        Returns:
            Analysis of the image as string

        Raises:
            Exception: If vision analysis fails
        """
        start_time = datetime.now()

        logger.bind(event="VISION_AGENT|ANALYZE_START", user_id=user_id).info(
            "Starting image analysis",
            image_url=image_url[:100],
            query_preview=query[:100],
        )

        try:
            # Build multimodal prompt (OpenAI Agents SDK format)
            # Uses input_text + input_image content types
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "input_text", "text": query},
                        {"type": "input_image", "image_url": image_url},
                    ],
                }
            ]

            # Execute via OpenAI Agents SDK Runner (static method)
            result = await Runner.run(
                self.agent,
                messages,  # type: ignore[arg-type]
                session=None,
            )

            # Extract analysis
            analysis = result.final_output

            # Calculate processing time
            processing_time = (datetime.now() - start_time).total_seconds()

            logger.bind(event="VISION_AGENT|ANALYZE_SUCCESS", user_id=user_id).info(
                "Image analysis completed",
                processing_time=processing_time,
                response_length=len(analysis),
            )

            return analysis

        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.bind(event="VISION_AGENT|ANALYZE_ERROR", user_id=user_id).exception(
                "Vision analysis failed",
                error_type=type(e).__name__,
                error_msg=str(e),
                processing_time=processing_time,
            )
            raise

    def get_agent(self) -> Agent:
        """Get the underlying Agent instance for tool registration."""
        return self.agent

    # VOXY Orchestrator Integration (Dual-Path Support)

    def process_tool_call(
        self,
        image_url: str,
        query: str = "Analise esta imagem",
        analysis_type: str = "general",
        detail_level: str = "standard",
        specific_questions: Optional[list[str]] = None,
    ) -> str:
        """
        Process tool call from VOXY Orchestrator (synchronous wrapper).

        Enables dual-path: PATH1 (direct) or PATH2 (via VOXY decision).
        Uses nest_asyncio for sync/async compatibility.
        """
        # Build enhanced query with analysis parameters
        enhanced_query = self._enhance_query(
            query, analysis_type, detail_level, specific_questions
        )

        # Run async analyze_image in sync context
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        analysis = loop.run_until_complete(
            self.analyze_image(
                image_url=image_url, query=enhanced_query, user_id="voxy_orchestrator"
            )
        )

        # Return structured format for VOXY
        return (
            f"[VISION_ANALYSIS]\n"
            f"Tipo: {analysis_type}\n"
            f"NÃ­vel: {detail_level}\n\n"
            f"AnÃ¡lise:\n{analysis}\n\n"
            f"[/VISION_ANALYSIS]"
        )

    def _enhance_query(
        self,
        query: str,
        analysis_type: str,
        detail_level: str,
        specific_questions: Optional[list[str]],
    ) -> str:
        """Enhance query with analysis type, detail level, and specific questions."""
        enhanced = query

        # Add analysis type context
        type_contexts = {
            "ocr": "Foque na extraÃ§Ã£o e transcriÃ§Ã£o de todo texto visÃ­vel.",
            "technical": "Concentre-se em aspectos tÃ©cnicos: qualidade, problemas, elementos especÃ­ficos.",
            "artistic": "Analise aspectos artÃ­sticos: composiÃ§Ã£o, cores, estilo, tÃ©cnica.",
            "document": "Trate como documento: extraia informaÃ§Ãµes estruturadas.",
        }
        if analysis_type in type_contexts:
            enhanced += f"\n{type_contexts[analysis_type]}"

        # Add detail level context
        detail_contexts = {
            "comprehensive": "\nForneÃ§a anÃ¡lise extremamente detalhada com explicaÃ§Ã£o passo a passo.",
            "detailed": "\nForneÃ§a anÃ¡lise detalhada com contexto e explicaÃ§Ãµes.",
            "basic": None,  # Handled separately
        }
        if detail_level == "basic":
            enhanced = f"Responda de forma direta e concisa: {query}"
        elif detail_level in detail_contexts:
            context = detail_contexts[detail_level]
            if context:
                enhanced += context

        # Add specific questions
        if specific_questions:
            questions = "\n".join(f"- {q}" for q in specific_questions)
            enhanced += f"\n\nQuestÃµes especÃ­ficas:\n{questions}"

        return enhanced


# Global singleton (equal to other agents)
_vision_agent = None


def get_vision_agent() -> VisionAgent:
    """
    Get the global vision agent instance using lazy initialization.

    Thread-safe singleton pattern ensures single instance across application.
    """
    global _vision_agent
    if _vision_agent is None:
        _vision_agent = VisionAgent()
    return _vision_agent
